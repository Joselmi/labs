{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d941f5d",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aca53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "HADOOP_START_FROM_SCRATCH = True\n",
    "DOCKER_INTERNAL_HOST = \"host.docker.internal\"\n",
    "DOCKER_DNS = [\"10.15.20.1\"]\n",
    "\n",
    "HADOOP_NAMENODE_HOSTNAME = \"namenode.mavasbel.vpn.itam.mx\"\n",
    "HADOOP_NAMENODE_IP = \"10.15.20.2\"\n",
    "HADOOP_NAMENODE_PORT = 8020\n",
    "HADOOP_NAMENODE_WEBUI_PORT = 9870\n",
    "\n",
    "HADOOP_RESOURCEMANAGER_HOSTNAME = \"resourcemanager.mavasbel.vpn.itam.mx\"\n",
    "HADOOP_RESOURCEMANAGER_IP = \"10.15.20.2\"\n",
    "HADOOP_RESOURCEMANAGER_WEBUI_PORT = 8088\n",
    "HADOOP_RESOURCEMANAGER_RPC_APP_MANAGER_PORT = 8032\n",
    "HADOOP_RESOURCEMANAGER_TRACKER_PORT = 8031\n",
    "HADOOP_RESOURCEMANAGER_SCHEDULER_PORT = 8030\n",
    "HADOOP_RESOURCEMANAGER_ADMIN_PORT = 8033\n",
    "\n",
    "HADOOP_REPLICATION = 3\n",
    "HADOOP_NUM_WORKERS = 3\n",
    "\n",
    "HADOOP_DATANODE_IPS = [\"10.15.20.2\"] * 3\n",
    "HADOOP_DATANODE_NAMES = [f\"datanode-{i+1}\" for i in range(HADOOP_NUM_WORKERS)]\n",
    "HADOOP_DATANODE_HOSTNAMES = [\n",
    "    f\"{HADOOP_DATANODE_NAMES[i]}.mavasbel.vpn.itam.mx\"\n",
    "    for i in range(HADOOP_NUM_WORKERS)\n",
    "]\n",
    "HADOOP_DATANODE_WEBUI_PORTS = [9864 + (i * 10) for i in range(HADOOP_NUM_WORKERS)]\n",
    "HADOOP_DATANODE_TRANSFER_PORTS = [9866 + (i * 10) for i in range(HADOOP_NUM_WORKERS)]\n",
    "HADOOP_DATANODE_IPC_PORTS = [6867 + (i * 10) for i in range(HADOOP_NUM_WORKERS)]\n",
    "\n",
    "HADOOP_NODEMANAGER_IPS = [\"10.15.20.2\"] * 3\n",
    "HADOOP_NODEMANAGER_NAMES = [f\"nodemanager-{i+1}\" for i in range(HADOOP_NUM_WORKERS)]\n",
    "HADOOP_NODEMANAGER_HOSTNAMES = [\n",
    "    f\"{HADOOP_NODEMANAGER_NAMES[i]}.mavasbel.vpn.itam.mx\"\n",
    "    for i in range(HADOOP_NUM_WORKERS)\n",
    "]\n",
    "HADOOP_NODEMANAGER_WEBUI_PORTS = [8050 + (i * 10) for i in range(HADOOP_NUM_WORKERS)]\n",
    "HADOOP_NODEMANAGER_RPC_PORTS = [8051 + (i * 10) for i in range(HADOOP_NUM_WORKERS)]\n",
    "\n",
    "HADOOP_WORKDIR = \"/opt/hadoop/work-dir\"\n",
    "HADOOP_NAMENODE_NAMEDIR = \"/opt/hadoop/dfs/name\"\n",
    "HADOOP_DATANODE_DATADIR = \"/opt/hadoop/dfs/data\"\n",
    "\n",
    "HADOOP_HDFS_DATADIR = \"/opt/hadoop/work-dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80b128f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "LOCALHOST_WORKDIR = f\"{os.path.join(os.path.relpath(Path.cwd()))}\"\n",
    "DOCKER_MOUNTDIR = os.path.join(LOCALHOST_WORKDIR, \"mount\")\n",
    "\n",
    "Path(DOCKER_MOUNTDIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad343b0",
   "metadata": {},
   "source": [
    "# Stop hadoop-cluster.docker-compose.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29759a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Container nodemanager-3  Stopping\n",
      " Container nodemanager-1  Stopping\n",
      " Container nodemanager-2  Stopping\n",
      " Container nodemanager-3  Stopped\n",
      " Container nodemanager-3  Removing\n",
      " Container nodemanager-3  Removed\n",
      " Container datanode-3  Stopping\n",
      " Container nodemanager-2  Stopped\n",
      " Container nodemanager-2  Removing\n",
      " Container nodemanager-2  Removed\n",
      " Container datanode-2  Stopping\n",
      " Container nodemanager-1  Stopped\n",
      " Container nodemanager-1  Removing\n",
      " Container nodemanager-1  Removed\n",
      " Container datanode-1  Stopping\n",
      " Container resourcemanager  Stopping\n",
      " Container datanode-3  Stopped\n",
      " Container datanode-3  Removing\n",
      " Container datanode-3  Removed\n",
      " Container datanode-2  Stopped\n",
      " Container datanode-2  Removing\n",
      " Container datanode-2  Removed\n",
      " Container resourcemanager  Stopped\n",
      " Container resourcemanager  Removing\n",
      " Container resourcemanager  Removed\n",
      " Container datanode-1  Stopped\n",
      " Container datanode-1  Removing\n",
      " Container datanode-1  Removed\n",
      " Container namenode  Stopping\n",
      " Container namenode  Stopped\n",
      " Container namenode  Removing\n",
      " Container namenode  Removed\n",
      " Network hadoop-cluster_hadoop-cluster  Removing\n",
      " Network hadoop-cluster_hadoop-cluster  Removed\n"
     ]
    }
   ],
   "source": [
    "!docker compose -f hadoop-cluster.docker-compose.yml down -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1825f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "if HADOOP_START_FROM_SCRATCH:\n",
    "    shutil.rmtree(DOCKER_MOUNTDIR, ignore_errors=True)\n",
    "    Path(DOCKER_MOUNTDIR).mkdir(parents=True, exist_ok=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde5aca6",
   "metadata": {},
   "source": [
    "# Start hadoop-cluster.docker-compose.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fe4caf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created: c:\\Users\\Marco\\Documents\\ITAM\\2026 - Bases de Datos No Relacionales\\code\\hadoop\\hadoop-cluster.docker-compose.yml\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```yaml\n",
       "name: hadoop-cluster\n",
       "networks:\n",
       "    hadoop-cluster:\n",
       "        driver: bridge\n",
       "services:\n",
       "    namenode:\n",
       "        image: apache/hadoop:3.4.2\n",
       "        container_name: namenode\n",
       "        command:\n",
       "        - bash\n",
       "        - -c\n",
       "        - sudo chown -R $(id -u hadoop):$(id -g hadoop) /opt/hadoop/work-dir && sudo\n",
       "            mkdir -p /opt/hadoop/dfs/name && sudo chown -R $(id -u hadoop):$(id -g\n",
       "            hadoop) /opt/hadoop/dfs/name && if [ ! -d /opt/hadoop/dfs/name/current\n",
       "            ]; then hdfs namenode -Ddfs.namenode.name.dir=/opt/hadoop/dfs/name -format\n",
       "            -force; fi && hdfs namenode -Dfs.defaultFS=hdfs://namenode.mavasbel.vpn.itam.mx:8020\n",
       "            -Ddfs.replication=3 -Ddfs.namenode.name.dir=/opt/hadoop/dfs/name -Ddfs.namenode.name.dir.perm=775\n",
       "            -Ddfs.permissions.enabled=false -Ddfs.namenode.rpc-address=0.0.0.0:8020\n",
       "            -Ddfs.namenode.http-address=0.0.0.0:9870\n",
       "        env_file:\n",
       "        - envs/common.env\n",
       "        volumes:\n",
       "        - .\\mount\\namenode\\work-dir:/opt/hadoop/work-dir\n",
       "        - .\\mount\\namenode\\name-dir:/opt/hadoop/dfs/name\n",
       "        networks:\n",
       "        - hadoop-cluster\n",
       "        hostname: namenode.mavasbel.vpn.itam.mx\n",
       "        ports:\n",
       "        - 8020:8020\n",
       "        - 9870:9870\n",
       "        extra_hosts:\n",
       "        - host.docker.internal:host-gateway\n",
       "        dns: &id001\n",
       "        - 10.15.20.1\n",
       "        deploy:\n",
       "            resources:\n",
       "                limits:\n",
       "                    memory: 2048M\n",
       "    resourcemanager:\n",
       "        image: apache/hadoop:3.4.2\n",
       "        container_name: resourcemanager\n",
       "        command:\n",
       "        - bash\n",
       "        - -c\n",
       "        - sudo chown -R $(id -u hadoop):$(id -g hadoop) /opt/hadoop/work-dir && yarn\n",
       "            resourcemanager -Dfs.defaultFS=hdfs://namenode.mavasbel.vpn.itam.mx:8020\n",
       "            -Dyarn.resourcemanager.hostname=resourcemanager.mavasbel.vpn.itam.mx -Dyarn.resourcemanager.webapp.address=0.0.0.0:8088\n",
       "            -Dyarn.resourcemanager.address=0.0.0.0:8032 -Dyarn.resourcemanager.scheduler.address=0.0.0.0:8030\n",
       "            -Dyarn.resourcemanager.resource-tracker.address=0.0.0.0:8031 -Dyarn.resourcemanager.admin.address=0.0.0.0:8033\n",
       "        env_file:\n",
       "        - envs/common.env\n",
       "        volumes:\n",
       "        - .\\mount\\resourcemanager\\work-dir:/opt/hadoop/work-dir\n",
       "        networks:\n",
       "        - hadoop-cluster\n",
       "        hostname: resourcemanager.mavasbel.vpn.itam.mx\n",
       "        ports:\n",
       "        - 8088:8088\n",
       "        - 8032:8032\n",
       "        - 8030:8030\n",
       "        - 8031:8031\n",
       "        - 8033:8033\n",
       "        extra_hosts:\n",
       "        - host.docker.internal:host-gateway\n",
       "        dns: *id001\n",
       "        depends_on:\n",
       "            namenode:\n",
       "                condition: service_started\n",
       "        deploy:\n",
       "            resources:\n",
       "                limits:\n",
       "                    memory: 2048M\n",
       "    datanode-1:\n",
       "        image: apache/hadoop:3.4.2\n",
       "        container_name: datanode-1\n",
       "        command:\n",
       "        - bash\n",
       "        - -c\n",
       "        - sudo chown -R $(id -u hadoop):$(id -g hadoop) /opt/hadoop/work-dir && sudo\n",
       "            mkdir -p /opt/hadoop/dfs/data && sudo chown -R $(id -u hadoop):$(id -g\n",
       "            hadoop) /opt/hadoop/dfs/data && hdfs datanode -Dfs.defaultFS=hdfs://namenode.mavasbel.vpn.itam.mx:8020\n",
       "            -Ddfs.datanode.data.dir=/opt/hadoop/dfs/data -Ddfs.datanode.data.dir.perm=775\n",
       "            -Ddfs.permissions.enabled=false -Ddfs.datanode.address=0.0.0.0:9866 -Ddfs.datanode.http.address=0.0.0.0:9864\n",
       "            -Ddfs.datanode.ipc.address=0.0.0.0:6867 -Ddfs.datanode.use.datanode.hostname=true\n",
       "        env_file:\n",
       "        - envs/common.env\n",
       "        volumes:\n",
       "        - .\\mount\\datanode-1\\work-dir:/opt/hadoop/work-dir\n",
       "        - .\\mount\\datanode-1\\data-dir:/opt/hadoop/dfs/data\n",
       "        networks:\n",
       "        - hadoop-cluster\n",
       "        hostname: datanode-1.mavasbel.vpn.itam.mx\n",
       "        ports:\n",
       "        - 9864:9864\n",
       "        - 9866:9866\n",
       "        - 6867:6867\n",
       "        extra_hosts:\n",
       "        - host.docker.internal:host-gateway\n",
       "        dns: *id001\n",
       "        deploy:\n",
       "            resources:\n",
       "                limits:\n",
       "                    memory: 2048M\n",
       "        depends_on:\n",
       "            namenode:\n",
       "                condition: service_started\n",
       "            resourcemanager:\n",
       "                condition: service_started\n",
       "    nodemanager-1:\n",
       "        image: apache/hadoop:3.4.2\n",
       "        container_name: nodemanager-1\n",
       "        command:\n",
       "        - bash\n",
       "        - -c\n",
       "        - sudo chown -R $(id -u hadoop):$(id -g hadoop) /opt/hadoop/work-dir && yarn\n",
       "            nodemanager -Dyarn.resourcemanager.hostname=resourcemanager.mavasbel.vpn.itam.mx\n",
       "            -Dyarn.nodemanager.aux-services=mapreduce_shuffle -Dyarn.resourcemanager.resource-tracker.address=resourcemanager.mavasbel.vpn.itam.mx:8031\n",
       "            -Dyarn.nodemanager.address=0.0.0.0:8051 -Dyarn.nodemanager.webapp.address=0.0.0.0:9864\n",
       "        env_file:\n",
       "        - envs/common.env\n",
       "        volumes:\n",
       "        - .\\mount\\nodemanager-1\\work-dir:/opt/hadoop/work-dir\n",
       "        networks:\n",
       "        - hadoop-cluster\n",
       "        hostname: nodemanager-1.mavasbel.vpn.itam.mx\n",
       "        ports:\n",
       "        - 8050:8050\n",
       "        - 8051:8051\n",
       "        extra_hosts:\n",
       "        - host.docker.internal:host-gateway\n",
       "        dns: *id001\n",
       "        deploy:\n",
       "            resources:\n",
       "                limits:\n",
       "                    memory: 2048M\n",
       "        depends_on:\n",
       "            namenode:\n",
       "                condition: service_started\n",
       "            resourcemanager:\n",
       "                condition: service_started\n",
       "            datanode-1:\n",
       "                condition: service_started\n",
       "            datanode-2:\n",
       "                condition: service_started\n",
       "            datanode-3:\n",
       "                condition: service_started\n",
       "    datanode-2:\n",
       "        image: apache/hadoop:3.4.2\n",
       "        container_name: datanode-2\n",
       "        command:\n",
       "        - bash\n",
       "        - -c\n",
       "        - sudo chown -R $(id -u hadoop):$(id -g hadoop) /opt/hadoop/work-dir && sudo\n",
       "            mkdir -p /opt/hadoop/dfs/data && sudo chown -R $(id -u hadoop):$(id -g\n",
       "            hadoop) /opt/hadoop/dfs/data && hdfs datanode -Dfs.defaultFS=hdfs://namenode.mavasbel.vpn.itam.mx:8020\n",
       "            -Ddfs.datanode.data.dir=/opt/hadoop/dfs/data -Ddfs.datanode.data.dir.perm=775\n",
       "            -Ddfs.permissions.enabled=false -Ddfs.datanode.address=0.0.0.0:9876 -Ddfs.datanode.http.address=0.0.0.0:9874\n",
       "            -Ddfs.datanode.ipc.address=0.0.0.0:6877 -Ddfs.datanode.use.datanode.hostname=true\n",
       "        env_file:\n",
       "        - envs/common.env\n",
       "        volumes:\n",
       "        - .\\mount\\datanode-2\\work-dir:/opt/hadoop/work-dir\n",
       "        - .\\mount\\datanode-2\\data-dir:/opt/hadoop/dfs/data\n",
       "        networks:\n",
       "        - hadoop-cluster\n",
       "        hostname: datanode-2.mavasbel.vpn.itam.mx\n",
       "        ports:\n",
       "        - 9874:9874\n",
       "        - 9876:9876\n",
       "        - 6877:6877\n",
       "        extra_hosts:\n",
       "        - host.docker.internal:host-gateway\n",
       "        dns: *id001\n",
       "        deploy:\n",
       "            resources:\n",
       "                limits:\n",
       "                    memory: 2048M\n",
       "        depends_on:\n",
       "            namenode:\n",
       "                condition: service_started\n",
       "            resourcemanager:\n",
       "                condition: service_started\n",
       "            datanode-1:\n",
       "                condition: service_started\n",
       "    nodemanager-2:\n",
       "        image: apache/hadoop:3.4.2\n",
       "        container_name: nodemanager-2\n",
       "        command:\n",
       "        - bash\n",
       "        - -c\n",
       "        - sudo chown -R $(id -u hadoop):$(id -g hadoop) /opt/hadoop/work-dir && yarn\n",
       "            nodemanager -Dyarn.resourcemanager.hostname=resourcemanager.mavasbel.vpn.itam.mx\n",
       "            -Dyarn.nodemanager.aux-services=mapreduce_shuffle -Dyarn.resourcemanager.resource-tracker.address=resourcemanager.mavasbel.vpn.itam.mx:8031\n",
       "            -Dyarn.nodemanager.address=0.0.0.0:8061 -Dyarn.nodemanager.webapp.address=0.0.0.0:9874\n",
       "        env_file:\n",
       "        - envs/common.env\n",
       "        volumes:\n",
       "        - .\\mount\\nodemanager-2\\work-dir:/opt/hadoop/work-dir\n",
       "        networks:\n",
       "        - hadoop-cluster\n",
       "        hostname: nodemanager-2.mavasbel.vpn.itam.mx\n",
       "        ports:\n",
       "        - 8060:8060\n",
       "        - 8061:8061\n",
       "        extra_hosts:\n",
       "        - host.docker.internal:host-gateway\n",
       "        dns: *id001\n",
       "        deploy:\n",
       "            resources:\n",
       "                limits:\n",
       "                    memory: 2048M\n",
       "        depends_on:\n",
       "            namenode:\n",
       "                condition: service_started\n",
       "            resourcemanager:\n",
       "                condition: service_started\n",
       "            datanode-1:\n",
       "                condition: service_started\n",
       "            datanode-2:\n",
       "                condition: service_started\n",
       "            datanode-3:\n",
       "                condition: service_started\n",
       "            nodemanager-1:\n",
       "                condition: service_started\n",
       "    datanode-3:\n",
       "        image: apache/hadoop:3.4.2\n",
       "        container_name: datanode-3\n",
       "        command:\n",
       "        - bash\n",
       "        - -c\n",
       "        - sudo chown -R $(id -u hadoop):$(id -g hadoop) /opt/hadoop/work-dir && sudo\n",
       "            mkdir -p /opt/hadoop/dfs/data && sudo chown -R $(id -u hadoop):$(id -g\n",
       "            hadoop) /opt/hadoop/dfs/data && hdfs datanode -Dfs.defaultFS=hdfs://namenode.mavasbel.vpn.itam.mx:8020\n",
       "            -Ddfs.datanode.data.dir=/opt/hadoop/dfs/data -Ddfs.datanode.data.dir.perm=775\n",
       "            -Ddfs.permissions.enabled=false -Ddfs.datanode.address=0.0.0.0:9886 -Ddfs.datanode.http.address=0.0.0.0:9884\n",
       "            -Ddfs.datanode.ipc.address=0.0.0.0:6887 -Ddfs.datanode.use.datanode.hostname=true\n",
       "        env_file:\n",
       "        - envs/common.env\n",
       "        volumes:\n",
       "        - .\\mount\\datanode-3\\work-dir:/opt/hadoop/work-dir\n",
       "        - .\\mount\\datanode-3\\data-dir:/opt/hadoop/dfs/data\n",
       "        networks:\n",
       "        - hadoop-cluster\n",
       "        hostname: datanode-3.mavasbel.vpn.itam.mx\n",
       "        ports:\n",
       "        - 9884:9884\n",
       "        - 9886:9886\n",
       "        - 6887:6887\n",
       "        extra_hosts:\n",
       "        - host.docker.internal:host-gateway\n",
       "        dns: *id001\n",
       "        deploy:\n",
       "            resources:\n",
       "                limits:\n",
       "                    memory: 2048M\n",
       "        depends_on:\n",
       "            namenode:\n",
       "                condition: service_started\n",
       "            resourcemanager:\n",
       "                condition: service_started\n",
       "            datanode-1:\n",
       "                condition: service_started\n",
       "            datanode-2:\n",
       "                condition: service_started\n",
       "    nodemanager-3:\n",
       "        image: apache/hadoop:3.4.2\n",
       "        container_name: nodemanager-3\n",
       "        command:\n",
       "        - bash\n",
       "        - -c\n",
       "        - sudo chown -R $(id -u hadoop):$(id -g hadoop) /opt/hadoop/work-dir && yarn\n",
       "            nodemanager -Dyarn.resourcemanager.hostname=resourcemanager.mavasbel.vpn.itam.mx\n",
       "            -Dyarn.nodemanager.aux-services=mapreduce_shuffle -Dyarn.resourcemanager.resource-tracker.address=resourcemanager.mavasbel.vpn.itam.mx:8031\n",
       "            -Dyarn.nodemanager.address=0.0.0.0:8071 -Dyarn.nodemanager.webapp.address=0.0.0.0:9884\n",
       "        env_file:\n",
       "        - envs/common.env\n",
       "        volumes:\n",
       "        - .\\mount\\nodemanager-3\\work-dir:/opt/hadoop/work-dir\n",
       "        networks:\n",
       "        - hadoop-cluster\n",
       "        hostname: nodemanager-3.mavasbel.vpn.itam.mx\n",
       "        ports:\n",
       "        - 8070:8070\n",
       "        - 8071:8071\n",
       "        extra_hosts:\n",
       "        - host.docker.internal:host-gateway\n",
       "        dns: *id001\n",
       "        deploy:\n",
       "            resources:\n",
       "                limits:\n",
       "                    memory: 2048M\n",
       "        depends_on:\n",
       "            namenode:\n",
       "                condition: service_started\n",
       "            resourcemanager:\n",
       "                condition: service_started\n",
       "            datanode-1:\n",
       "                condition: service_started\n",
       "            datanode-2:\n",
       "                condition: service_started\n",
       "            datanode-3:\n",
       "                condition: service_started\n",
       "            nodemanager-1:\n",
       "                condition: service_started\n",
       "            nodemanager-2:\n",
       "                condition: service_started\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "env_content = {\n",
    "    \"HADOOP_HOME\": \"/opt/hadoop\",\n",
    "    \"HADOOP_HEAPSIZE_MAX\": \"1536\",\n",
    "    \"YARN_HEAPSIZE\": \"1536\",\n",
    "    \"MAPRED-SITE.XML_mapreduce.application.classpath\": \":\".join(\n",
    "        [\n",
    "            \"$HADOOP_HOME/share/hadoop/mapreduce/*\",\n",
    "            \"$HADOOP_HOME/share/hadoop/common/*\",\n",
    "            \"$HADOOP_HOME/share/hadoop/common/lib/*\",\n",
    "            \"$HADOOP_HOME/share/hadoop/hdfs/*\",\n",
    "            \"$HADOOP_HOME/share/hadoop/hdfs/lib/*\",\n",
    "            \"$HADOOP_HOME/share/hadoop/yarn/*\",\n",
    "            \"$HADOOP_HOME/share/hadoop/yarn/lib/*\",\n",
    "        ]\n",
    "    ),\n",
    "    \"MAPRED-SITE.XML_mapreduce.framework.name\": \"yarn\",\n",
    "    \"CORE-SITE.XML_dfs.replication\": HADOOP_REPLICATION,\n",
    "    \"CORE-SITE.XML_fs.defaultFS\": f\"hdfs://{HADOOP_NAMENODE_HOSTNAME}:{HADOOP_NAMENODE_PORT}\",\n",
    "    \"YARN-SITE.XML_yarn.resourcemanager.hostname\": HADOOP_RESOURCEMANAGER_HOSTNAME,\n",
    "}\n",
    "with open(\"envs/common.env\", \"w\") as f:\n",
    "    for key, value in env_content.items():\n",
    "        f.write(f\"{key}={value}\\n\")\n",
    "\n",
    "hadoop_compose_file_name = \"hadoop-cluster.docker-compose.yml\"\n",
    "hadoop_compose_dict = {\n",
    "    \"name\": \"hadoop-cluster\",\n",
    "    \"networks\": {\"hadoop-cluster\": {\"driver\": \"bridge\"}},\n",
    "    \"services\": {\n",
    "        \"namenode\": {\n",
    "            \"image\": \"apache/hadoop:3.4.2\",\n",
    "            \"container_name\": \"namenode\",\n",
    "            \"command\": [\n",
    "                \"bash\",\n",
    "                \"-c\",\n",
    "                \" \".join(\n",
    "                    [\n",
    "                        f\"sudo chown -R $(id -u hadoop):$(id -g hadoop) {HADOOP_WORKDIR} &&\",\n",
    "                        f\"sudo mkdir -p {HADOOP_NAMENODE_NAMEDIR} &&\",\n",
    "                        f\"sudo chown -R $(id -u hadoop):$(id -g hadoop) {HADOOP_NAMENODE_NAMEDIR} &&\",\n",
    "                        f\"if [ ! -d {HADOOP_NAMENODE_NAMEDIR}/current ]; then hdfs namenode -Ddfs.namenode.name.dir={HADOOP_NAMENODE_NAMEDIR} -format -force; fi &&\",\n",
    "                        \"hdfs\",\n",
    "                        \"namenode\",\n",
    "                        f\"-Dfs.defaultFS=hdfs://{HADOOP_NAMENODE_HOSTNAME}:{HADOOP_NAMENODE_PORT}\",\n",
    "                        f\"-Ddfs.replication={HADOOP_REPLICATION}\",\n",
    "                        f\"-Ddfs.namenode.name.dir={HADOOP_NAMENODE_NAMEDIR}\",\n",
    "                        \"-Ddfs.namenode.name.dir.perm=775\",\n",
    "                        \"-Ddfs.permissions.enabled=false\",\n",
    "                        f\"-Ddfs.namenode.rpc-address=0.0.0.0:{HADOOP_NAMENODE_PORT}\",\n",
    "                        f\"-Ddfs.namenode.http-address=0.0.0.0:{HADOOP_NAMENODE_WEBUI_PORT}\",\n",
    "                    ]\n",
    "                ),\n",
    "            ],\n",
    "            \"env_file\": [\"envs/common.env\"],\n",
    "            \"volumes\": [\n",
    "                f\"{os.path.join(DOCKER_MOUNTDIR,\"namenode\",\"work-dir\")}:{HADOOP_WORKDIR}\",\n",
    "                f\"{os.path.join(DOCKER_MOUNTDIR,\"namenode\",\"name-dir\")}:{HADOOP_NAMENODE_NAMEDIR}\",\n",
    "            ],\n",
    "            \"networks\": [\"hadoop-cluster\"],\n",
    "            \"hostname\": HADOOP_NAMENODE_HOSTNAME,\n",
    "            \"ports\": [\n",
    "                f\"{HADOOP_NAMENODE_PORT}:{HADOOP_NAMENODE_PORT}\",\n",
    "                f\"{HADOOP_NAMENODE_WEBUI_PORT}:{HADOOP_NAMENODE_WEBUI_PORT}\",\n",
    "            ],\n",
    "            \"extra_hosts\": [\n",
    "                f\"{DOCKER_INTERNAL_HOST}:host-gateway\",\n",
    "            ],\n",
    "            \"dns\": DOCKER_DNS,\n",
    "            \"deploy\": {\"resources\": {\"limits\": {\"memory\": \"2048M\"}}},\n",
    "        },\n",
    "        \"resourcemanager\": {\n",
    "            \"image\": \"apache/hadoop:3.4.2\",\n",
    "            \"container_name\": \"resourcemanager\",\n",
    "            \"command\": [\n",
    "                \"bash\",\n",
    "                \"-c\",\n",
    "                \" \".join(\n",
    "                    [\n",
    "                        f\"sudo chown -R $(id -u hadoop):$(id -g hadoop) {HADOOP_WORKDIR} &&\",\n",
    "                        \"yarn\",\n",
    "                        \"resourcemanager\",\n",
    "                        f\"-Dfs.defaultFS=hdfs://{HADOOP_NAMENODE_HOSTNAME}:{HADOOP_NAMENODE_PORT}\",\n",
    "                        f\"-Dyarn.resourcemanager.hostname={HADOOP_RESOURCEMANAGER_HOSTNAME}\",\n",
    "                        f\"-Dyarn.resourcemanager.webapp.address=0.0.0.0:{HADOOP_RESOURCEMANAGER_WEBUI_PORT}\",\n",
    "                        f\"-Dyarn.resourcemanager.address=0.0.0.0:{HADOOP_RESOURCEMANAGER_RPC_APP_MANAGER_PORT}\",\n",
    "                        f\"-Dyarn.resourcemanager.scheduler.address=0.0.0.0:{HADOOP_RESOURCEMANAGER_SCHEDULER_PORT}\",\n",
    "                        f\"-Dyarn.resourcemanager.resource-tracker.address=0.0.0.0:{HADOOP_RESOURCEMANAGER_TRACKER_PORT}\",\n",
    "                        f\"-Dyarn.resourcemanager.admin.address=0.0.0.0:{HADOOP_RESOURCEMANAGER_ADMIN_PORT}\",\n",
    "                    ]\n",
    "                ),\n",
    "            ],\n",
    "            \"env_file\": [\"envs/common.env\"],\n",
    "            \"volumes\": [\n",
    "                f\"{os.path.join(DOCKER_MOUNTDIR,\"resourcemanager\",\"work-dir\")}:{HADOOP_WORKDIR}\",\n",
    "            ],\n",
    "            \"networks\": [\"hadoop-cluster\"],\n",
    "            \"hostname\": HADOOP_RESOURCEMANAGER_HOSTNAME,\n",
    "            \"ports\": [\n",
    "                f\"{HADOOP_RESOURCEMANAGER_WEBUI_PORT}:{HADOOP_RESOURCEMANAGER_WEBUI_PORT}\",\n",
    "                f\"{HADOOP_RESOURCEMANAGER_RPC_APP_MANAGER_PORT}:{HADOOP_RESOURCEMANAGER_RPC_APP_MANAGER_PORT}\",\n",
    "                f\"{HADOOP_RESOURCEMANAGER_SCHEDULER_PORT}:{HADOOP_RESOURCEMANAGER_SCHEDULER_PORT}\",\n",
    "                f\"{HADOOP_RESOURCEMANAGER_TRACKER_PORT}:{HADOOP_RESOURCEMANAGER_TRACKER_PORT}\",\n",
    "                f\"{HADOOP_RESOURCEMANAGER_ADMIN_PORT}:{HADOOP_RESOURCEMANAGER_ADMIN_PORT}\",\n",
    "            ],\n",
    "            \"extra_hosts\": [\n",
    "                f\"{DOCKER_INTERNAL_HOST}:host-gateway\",\n",
    "            ],\n",
    "            \"dns\": DOCKER_DNS,\n",
    "            \"depends_on\": {\"namenode\": {\"condition\": \"service_started\"}},\n",
    "            \"deploy\": {\"resources\": {\"limits\": {\"memory\": \"2048M\"}}},\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# Programmatically add DataNodes and NodeManagers\n",
    "for i in range(0, HADOOP_NUM_WORKERS):\n",
    "\n",
    "    hadoop_compose_dict[\"services\"][HADOOP_DATANODE_NAMES[i]] = {\n",
    "        \"image\": \"apache/hadoop:3.4.2\",\n",
    "        \"container_name\": HADOOP_DATANODE_NAMES[i],\n",
    "        \"command\": [\n",
    "            \"bash\",\n",
    "            \"-c\",\n",
    "            \" \".join(\n",
    "                [\n",
    "                    f\"sudo chown -R $(id -u hadoop):$(id -g hadoop) {HADOOP_WORKDIR} &&\",\n",
    "                    f\"sudo mkdir -p {HADOOP_DATANODE_DATADIR} &&\",\n",
    "                    f\"sudo chown -R $(id -u hadoop):$(id -g hadoop) {HADOOP_DATANODE_DATADIR} &&\",\n",
    "                    \"hdfs\",\n",
    "                    \"datanode\",\n",
    "                    f\"-Dfs.defaultFS=hdfs://{HADOOP_NAMENODE_HOSTNAME}:{HADOOP_NAMENODE_PORT}\",\n",
    "                    f\"-Ddfs.datanode.data.dir={HADOOP_DATANODE_DATADIR}\",\n",
    "                    \"-Ddfs.datanode.data.dir.perm=775\",\n",
    "                    \"-Ddfs.permissions.enabled=false\",\n",
    "                    f\"-Ddfs.datanode.address=0.0.0.0:{HADOOP_DATANODE_TRANSFER_PORTS[i]}\",\n",
    "                    f\"-Ddfs.datanode.http.address=0.0.0.0:{HADOOP_DATANODE_WEBUI_PORTS[i]}\",\n",
    "                    f\"-Ddfs.datanode.ipc.address=0.0.0.0:{HADOOP_DATANODE_IPC_PORTS[i]}\",\n",
    "                    \"-Ddfs.datanode.use.datanode.hostname=true\",\n",
    "                ]\n",
    "            ),\n",
    "        ],\n",
    "        \"env_file\": [\"envs/common.env\"],\n",
    "        \"volumes\": [\n",
    "            f\"{os.path.join(DOCKER_MOUNTDIR,HADOOP_DATANODE_NAMES[i],\"work-dir\")}:{HADOOP_WORKDIR}\",\n",
    "            f\"{os.path.join(DOCKER_MOUNTDIR,HADOOP_DATANODE_NAMES[i],\"data-dir\")}:{HADOOP_DATANODE_DATADIR}\",\n",
    "        ],\n",
    "        \"networks\": [\"hadoop-cluster\"],\n",
    "        \"hostname\": HADOOP_DATANODE_HOSTNAMES[i],\n",
    "        \"ports\": [\n",
    "            f\"{HADOOP_DATANODE_WEBUI_PORTS[i]}:{HADOOP_DATANODE_WEBUI_PORTS[i]}\",\n",
    "            f\"{HADOOP_DATANODE_TRANSFER_PORTS[i]}:{HADOOP_DATANODE_TRANSFER_PORTS[i]}\",\n",
    "            f\"{HADOOP_DATANODE_IPC_PORTS[i]}:{HADOOP_DATANODE_IPC_PORTS[i]}\",\n",
    "        ],\n",
    "        \"extra_hosts\": [\n",
    "            f\"{DOCKER_INTERNAL_HOST}:host-gateway\",\n",
    "        ],\n",
    "        \"dns\": DOCKER_DNS,\n",
    "        \"deploy\": {\"resources\": {\"limits\": {\"memory\": \"2048M\"}}},\n",
    "        \"depends_on\": {\n",
    "            \"namenode\": {\"condition\": \"service_started\"},\n",
    "            \"resourcemanager\": {\"condition\": \"service_started\"},\n",
    "        }\n",
    "        | {\n",
    "            HADOOP_DATANODE_NAMES[j]: {\"condition\": \"service_started\"}\n",
    "            for j in range(0, i)\n",
    "        },\n",
    "    }\n",
    "\n",
    "    hadoop_compose_dict[\"services\"][HADOOP_NODEMANAGER_NAMES[i]] = {\n",
    "        \"image\": \"apache/hadoop:3.4.2\",\n",
    "        \"container_name\": HADOOP_NODEMANAGER_NAMES[i],\n",
    "        \"command\": [\n",
    "            \"bash\",\n",
    "            \"-c\",\n",
    "            \" \".join(\n",
    "                [\n",
    "                    f\"sudo chown -R $(id -u hadoop):$(id -g hadoop) {HADOOP_WORKDIR} &&\",\n",
    "                    \"yarn\",\n",
    "                    \"nodemanager\",\n",
    "                    f\"-Dyarn.resourcemanager.hostname={HADOOP_RESOURCEMANAGER_HOSTNAME}\",\n",
    "                    \"-Dyarn.nodemanager.aux-services=mapreduce_shuffle\",\n",
    "                    f\"-Dyarn.resourcemanager.resource-tracker.address={HADOOP_RESOURCEMANAGER_HOSTNAME}:{HADOOP_RESOURCEMANAGER_TRACKER_PORT}\",\n",
    "                    f\"-Dyarn.nodemanager.address=0.0.0.0:{HADOOP_NODEMANAGER_RPC_PORTS[i]}\",\n",
    "                    f\"-Dyarn.nodemanager.webapp.address=0.0.0.0:{HADOOP_DATANODE_WEBUI_PORTS[i]}\",\n",
    "                ]\n",
    "            ),\n",
    "        ],\n",
    "        \"env_file\": [\"envs/common.env\"],\n",
    "        \"volumes\": [\n",
    "            f\"{os.path.join(DOCKER_MOUNTDIR,HADOOP_NODEMANAGER_NAMES[i],\"work-dir\")}:{HADOOP_WORKDIR}\",\n",
    "        ],\n",
    "        \"networks\": [\"hadoop-cluster\"],\n",
    "        \"hostname\": HADOOP_NODEMANAGER_HOSTNAMES[i],\n",
    "        \"ports\": [\n",
    "            f\"{HADOOP_NODEMANAGER_WEBUI_PORTS[i]}:{HADOOP_NODEMANAGER_WEBUI_PORTS[i]}\",\n",
    "            f\"{HADOOP_NODEMANAGER_RPC_PORTS[i]}:{HADOOP_NODEMANAGER_RPC_PORTS[i]}\",\n",
    "        ],\n",
    "        \"extra_hosts\": [\n",
    "            f\"{DOCKER_INTERNAL_HOST}:host-gateway\",\n",
    "        ],\n",
    "        \"dns\": DOCKER_DNS,\n",
    "        \"deploy\": {\"resources\": {\"limits\": {\"memory\": \"2048M\"}}},\n",
    "        \"depends_on\": {\n",
    "            \"namenode\": {\"condition\": \"service_started\"},\n",
    "            \"resourcemanager\": {\"condition\": \"service_started\"},\n",
    "        }\n",
    "        | {\n",
    "            HADOOP_DATANODE_NAMES[j]: {\"condition\": \"service_started\"}\n",
    "            for j in range(HADOOP_NUM_WORKERS)\n",
    "        }\n",
    "        | {\n",
    "            HADOOP_NODEMANAGER_NAMES[j]: {\"condition\": \"service_started\"}\n",
    "            for j in range(0, i)\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "hadoop_compose_yaml_contents = yaml.dump(\n",
    "    hadoop_compose_dict, default_flow_style=False, sort_keys=False, indent=4\n",
    ")\n",
    "with open(\n",
    "    os.path.join(LOCALHOST_WORKDIR, hadoop_compose_file_name), \"w\"\n",
    ") as hadoop_compose_file:\n",
    "    hadoop_compose_file.write(hadoop_compose_yaml_contents)\n",
    "print(\n",
    "    f\"Successfully created: {os.path.abspath(os.path.join(LOCALHOST_WORKDIR,hadoop_compose_file_name))}\"\n",
    ")\n",
    "display(Markdown(f\"```yaml\\n{hadoop_compose_yaml_contents}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63e0e702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Network hadoop-cluster_hadoop-cluster  Creating\n",
      " Network hadoop-cluster_hadoop-cluster  Created\n",
      " Container namenode  Creating\n",
      " Container namenode  Created\n",
      " Container resourcemanager  Creating\n",
      " Container resourcemanager  Created\n",
      " Container datanode-1  Creating\n",
      " Container datanode-1  Created\n",
      " Container datanode-2  Creating\n",
      " Container datanode-2  Created\n",
      " Container datanode-3  Creating\n",
      " Container datanode-3  Created\n",
      " Container nodemanager-1  Creating\n",
      " Container nodemanager-1  Created\n",
      " Container nodemanager-2  Creating\n",
      " Container nodemanager-2  Created\n",
      " Container nodemanager-3  Creating\n",
      " Container nodemanager-3  Created\n",
      " Container namenode  Starting\n",
      " Container namenode  Started\n",
      " Container resourcemanager  Starting\n",
      " Container resourcemanager  Started\n",
      " Container datanode-1  Starting\n",
      " Container datanode-1  Started\n",
      " Container datanode-2  Starting\n",
      " Container datanode-2  Started\n",
      " Container datanode-3  Starting\n",
      " Container datanode-3  Started\n",
      " Container nodemanager-1  Starting\n",
      " Container nodemanager-1  Started\n",
      " Container nodemanager-2  Starting\n",
      " Container nodemanager-2  Started\n",
      " Container nodemanager-3  Starting\n",
      " Container nodemanager-3  Started\n",
      " Container resourcemanager  Waiting\n",
      " Container nodemanager-3  Waiting\n",
      " Container datanode-2  Waiting\n",
      " Container namenode  Waiting\n",
      " Container nodemanager-1  Waiting\n",
      " Container datanode-3  Waiting\n",
      " Container datanode-1  Waiting\n",
      " Container nodemanager-2  Waiting\n",
      " Container datanode-1  Healthy\n",
      " Container nodemanager-3  Healthy\n",
      " Container resourcemanager  Healthy\n",
      " Container namenode  Healthy\n",
      " Container nodemanager-2  Healthy\n",
      " Container nodemanager-1  Healthy\n",
      " Container datanode-3  Healthy\n",
      " Container datanode-2  Healthy\n"
     ]
    }
   ],
   "source": [
    "!docker compose -f hadoop-cluster.docker-compose.yml up -d --wait"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "non-relational-dbs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
